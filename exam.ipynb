{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.concordance('woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motorcar = wn.synset('car.n.01')\n",
    "types_of_motorcar = motorcar.hyponyms()\n",
    "types_of_motorcar[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ConditionalFreqDist\n",
    "\n",
    "genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']\n",
    "modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
    "cfd = ConditionalFreqDist((genre, word)\n",
    "for genre in brown.categories()\n",
    "for word in brown.words(categories=genre))\n",
    "\n",
    "cfd.tabulate(conditions=genres, samples=modals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "for fileid in gutenberg.fileids():\n",
    "    num_chars = len(gutenberg.raw(fileid))\n",
    "    num_words = len(gutenberg.words(fileid))\n",
    "    num_sents = len(gutenberg.sents(fileid))\n",
    "    num_vocab = len(set(gutenberg.words(fileid)))\n",
    "    print(round(num_chars/num_words), round(num_words/num_sents),\n",
    "    round(num_words/num_vocab), fileid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "pattern='^data'\n",
    "text='data science'\n",
    "match=re.search(pattern,text)\n",
    "if match:\n",
    "    print('found')\n",
    "else:\n",
    "    print('not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern=re.compile(r'\\b(a|an|the)\\b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=['a apple','the kiwi','an banana']\n",
    "for word in text:\n",
    "    if pattern.search(word):\n",
    "        print(word)\n",
    "    else:\n",
    "        print(\"No match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Compile the regular expression\n",
    "pattern = re.compile(r'\\b(a|an|the)\\b',re.IGNORECASE)\n",
    "\n",
    "# The string to search\n",
    "text = \"The quick brown fox jumps over a lazy dog. An apple is on the tree.\"\n",
    "\n",
    "# Use the findall method to find all matches in the string\n",
    "matches = pattern.findall(text)\n",
    "\n",
    "# Print the matches\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# A capturing group\n",
    "pattern1 = re.compile(r'(a|an|the) \\w+')\n",
    "text = \"a car,  apple, the end\"\n",
    "matches1 = pattern1.findall(text)\n",
    "print(matches1)  # Outputs: ['a', 'an', 'the']\n",
    "\n",
    "# A non-capturing group\n",
    "pattern2 = re.compile(r'(?:a|an|the) (\\w+)')\n",
    "matches2 = pattern2.findall(text)\n",
    "print(matches2)  # Outputs: ['car', 'apple', 'end']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import udhr\n",
    "import nltk\n",
    "languages = ['Chickasaw', 'English', 'German_Deutsch',\n",
    "'Greenlandic_Inuktikut', 'Hungarian_Magyar', 'Ibibio_Efik']\n",
    "cfd = nltk.ConditionalFreqDist((lang, len(word)) for lang in languages for word in udhr.words(lang + '-Latin1'))\n",
    "cfd.plot(cumulative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = nltk.data.find('corpora/unicode_samples/polish-lat2.txt')\n",
    "\n",
    "f = open(path, encoding='latin2')\n",
    "for line in f:\n",
    "    line = line.strip()\n",
    "    print(line.encode('unicode_escape'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute '_taggers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m tagged_sentences \u001b[38;5;241m=\u001b[39m brown\u001b[38;5;241m.\u001b[39mtagged_sents(categories\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnews\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Fix: Use tagged_sents instead of tagged_words\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Train Unigram Tagger\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m tagger \u001b[38;5;241m=\u001b[39m \u001b[43mUnigramTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtagged_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackoff\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNN\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Fix: Pass tagged_sentences instead of tagged_words\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Test the tagger\u001b[39;00m\n\u001b[1;32m     12\u001b[0m sentense\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhello world\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/nltk/tag/sequential.py:363\u001b[0m, in \u001b[0;36mUnigramTagger.__init__\u001b[0;34m(self, train, model, backoff, cutoff, verbose)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, backoff\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, cutoff\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 363\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackoff\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcutoff\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/nltk/tag/sequential.py:293\u001b[0m, in \u001b[0;36mNgramTagger.__init__\u001b[0;34m(self, n, train, model, backoff, cutoff, verbose)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n \u001b[38;5;241m=\u001b[39m n\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params(train, model)\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackoff\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train:\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train(train, cutoff, verbose)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/nltk/tag/sequential.py:126\u001b[0m, in \u001b[0;36mContextTagger.__init__\u001b[0;34m(self, context_to_tag, backoff)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, context_to_tag, backoff\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    122\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m    :param context_to_tag: A dictionary mapping contexts to tags.\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m    :param backoff: The backoff tagger that should be used for this tagger.\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbackoff\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context_to_tag \u001b[38;5;241m=\u001b[39m context_to_tag \u001b[38;5;28;01mif\u001b[39;00m context_to_tag \u001b[38;5;28;01melse\u001b[39;00m {}\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/nltk/tag/sequential.py:50\u001b[0m, in \u001b[0;36mSequentialBackoffTagger.__init__\u001b[0;34m(self, backoff)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_taggers \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m]\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_taggers \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[43mbackoff\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_taggers\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute '_taggers'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.tag import UnigramTagger\n",
    "\n",
    "# Get tagged sentences from Brown Corpus\n",
    "tagged_sentences = brown.tagged_sents(categories='news')  # Fix: Use tagged_sents instead of tagged_words\n",
    "\n",
    "# Train Unigram Tagger\n",
    "tagger = UnigramTagger(tagged_sentences, backoff='NN')  # Fix: Pass tagged_sentences instead of tagged_words\n",
    "\n",
    "# Test the tagger\n",
    "sentense='hello world'\n",
    "tagger.tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('一', 'Neu'), ('友情', 'Nad'), ('嘉珍', 'Nba'), ...]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.sinica_treebank.tagged_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
